{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cheunghoyeung/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 6)\n",
      "(3563475, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201860</th>\n",
       "      <td>201860</td>\n",
       "      <td>303980</td>\n",
       "      <td>303981</td>\n",
       "      <td>How heavy is a full grown elephant? How is thi...</td>\n",
       "      <td>How heavy is a full grown elephant?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "201860  201860  303980  303981   \n",
       "\n",
       "                                                question1  \\\n",
       "201860  How heavy is a full grown elephant? How is thi...   \n",
       "\n",
       "                                  question2  is_duplicate  \n",
       "201860  How heavy is a full grown elephant?             1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1365233</th>\n",
       "      <td>1365233</td>\n",
       "      <td>What anarchy the most chaotic thing in the world?</td>\n",
       "      <td>Which is the most mysterious thing sion's in t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         test_id                                          question1  \\\n",
       "1365233  1365233  What anarchy the most chaotic thing in the world?   \n",
       "\n",
       "                                                 question2  \n",
       "1365233  Which is the most mysterious thing sion's in t...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['is_duplicate'].plot.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of duplicate in train set is 36.92%\n"
     ]
    }
   ],
   "source": [
    "percent = train['is_duplicate'].mean()*100\n",
    "print('The percentage of duplicate in train set is {0:.4g}%'.format(percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_qs = pd.Series(train['question1'].tolist()+train['question2'].tolist()).astype(str)\n",
    "train_qs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 376234),\n",
       " ('what', 310963),\n",
       " ('is', 268849),\n",
       " ('how', 219019),\n",
       " ('i', 213185),\n",
       " ('a', 210233),\n",
       " ('to', 204067),\n",
       " ('in', 194724),\n",
       " ('of', 158912),\n",
       " ('do', 154720),\n",
       " ('are', 145405),\n",
       " ('and', 132755),\n",
       " ('can', 113819),\n",
       " ('for', 101512),\n",
       " ('you', 84648),\n",
       " ('why', 75548),\n",
       " ('my', 70759),\n",
       " ('best', 69692),\n",
       " ('it', 60806),\n",
       " ('on', 59407)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.utils import simple_preprocess #text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_qs.tolist()\n",
    "sentences = filter(None, sentences) # remove empty strings\n",
    "#sentences = list(map(remove_stopwords,sentences)) #clean text \n",
    "sentences = list(map(simple_preprocess,sentences)) #clean text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = set('for a of the and to in on it is by'.split())\n",
    "sentences = [[word for word in sentence if word not in stoplist]\n",
    "          for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer \n",
    "snowball = SnowballStemmer(\"english\")\n",
    "stem_tokens = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    s = []\n",
    "    for word in sentence:\n",
    "        s.append(snowball.stem(word))\n",
    "    stem_tokens.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickle.dump(stem_tokens, open('stem_tokens.txt','wb'))\n",
    "stem_tokens = pickle.load(open('stem_tokens.txt','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "\n",
    "def get_wn_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pos = [nltk.pos_tag(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "lemma_tokens = []\n",
    "\n",
    "for sentence in sentence_pos:\n",
    "    s = []\n",
    "    for pair in sentence:\n",
    "        if  get_wn_pos(pair[1]) != '':\n",
    "            s.append(wnl.lemmatize(pair[0], get_wn_pos(pair[1])))\n",
    "        else:\n",
    "            s.append(wnl.lemmatize(pair[0]))\n",
    "    lemma_tokens.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(lemma_tokens, open('lemma_tokens.txt','wb'))\n",
    "lemma_tokens = pickle.load(open('lemma_tokens.txt','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorize = TfidfVectorizer()\n",
    "vect_word = TfidfVectorizer(lowercase=True,analyzer='word',ngram_range=(1,3),dtype=np.float32)\n",
    "vect_char = TfidfVectorizer(lowercase=True,analyzer='char',ngram_range=(3,6),dtype=np.float32)\n",
    "tfidf_matrix = vectorizer.fit_transform(train_qs)\n",
    "#tfidf_word_matrix = vect_word.fit_transform(train_qs)\n",
    "#tfidf_char_matrix = vect_char.fit_transform(train_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(808580, 2825037)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_word_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(808580, 2539587)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_char_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tfidf_pred = []\n",
    "start_time = time.time()\n",
    "for i in range(404290):\n",
    "    score = cosine_similarity(tfidf_matrix[i], tfidf_matrix[404290+i])\n",
    "    tfidf_pred.append(score[0][0])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickle.dump(tfidf_matrix, open('tfidf_matrix.txt','wb'))\n",
    "#pickle.dump(tfidf_word_matrix, open('tfidf_word_matrix.txt','wb'))\n",
    "#pickle.dump(tfidf_char_matrix, open('tfidf_char_matrix.txt','wb'))\n",
    "pickle.dump(tfidf_pred, open('tfidf_pred.txt','wb'))\n",
    "#tfidf_matrix = pickle.load(open('tfidf_matrix.txt','rb'))\n",
    "#tfidf_pred = pickle.load(open('tfidf_pred.txt','rb'))\n",
    "#tfidf_word_matrix = pickle.load(open('tfidf_word_matrix.txt','rb'))\n",
    "#tfidf_char_matrix = pickle.load(open('tfidf_char_matrix.txt','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF log_loss: 12.816658725868669\n",
      "TF-IDF accuracy: 0.6289198347720696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('TF-IDF log_loss:', log_loss(train['is_duplicate'], tfidf_pred))\n",
    "print('TF-IDF accuracy:', accuracy_score(train['is_duplicate'], \n",
    "                                         list(map(lambda x: round(x), tfidf_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = hstack([tfidf_word_matrix[0:404290], tfidf_char_matrix[0:404290], \n",
    "            tfidf_word_matrix[404290:], tfidf_char_matrix[404290:]])\n",
    "y = train['is_duplicate']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X,y, stratify=y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression log_loss: 0.5033287322410525\n",
      "Logistic Regression accuracy: 0.7597973741751012\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "logistic_pred = clf.predict(X_valid)\n",
    "logistic_pre_proba = clf.predict_proba(X_valid)\n",
    "print('Logistic Regression log_loss:', log_loss(y_valid, logistic_pre_proba))\n",
    "print('Logistic Regression accuracy:', accuracy_score(y_valid, logistic_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec(window=3, min_count=2, workers=4, alpha=0.025, min_alpha=0.025)\n",
    "d2v_model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 567.3281860351562 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "for epoch in range(10):\n",
    "    d2v_model.train(documents, total_examples=len(documents), epochs=1)\n",
    "    d2v_model.alpha -= 0.002                # decrease the learning rate\n",
    "    d2v_model.min_alpha = model.alpha       # fix the learning rate, no decay\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model.save('doc2vec.model')\n",
    "d2v_model = Doc2Vec.load('doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cheunghoyeung/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.563169002532959),\n",
       " (602108, 0.5296576023101807),\n",
       " (108486, 0.5231451392173767),\n",
       " (404290, 0.5165294408798218),\n",
       " (739041, 0.5136462450027466)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.docvecs.most_similar(positive=[model.infer_vector(sentences[0])],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cheunghoyeung/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.89288795"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.docvecs.n_similarity([0], [404290])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cheunghoyeung/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "doc2vec_pred = []\n",
    "for i in range(404290):\n",
    "    score = d2v_model.docvecs.similarity(i, 404290+i)\n",
    "    doc2vec_pred.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec AUC: 0.7028003809431244\n",
      "Doc2Vec accuracy: 0.6583492047787479\n"
     ]
    }
   ],
   "source": [
    "print('Doc2Vec log_loss:', log_loss(train['is_duplicate'], doc2vec_pred))\n",
    "print('Doc2Vec accuracy:', accuracy_score(train['is_duplicate'], list(map(lambda x: round(x), doc2vec_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(sentences, window=3, min_count=2, workers=4, alpha=0.025, min_alpha=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 64.77591300010681 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "for epoch in range(10):\n",
    "    w2v_model.train(sentences, total_examples=len(sentences), epochs=1)\n",
    "    w2v_model.alpha -= 0.002                # decrease the learning rate\n",
    "    w2v_model.min_alpha = w2v_model.alpha       # fix the learning rate, no decay\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_model.save('word2vec.zmodel.sav')\n",
    "w2v_model = Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_vector(words, model, num_features, index2word_set):\n",
    "    #function to average all words vectors in a given paragraph\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords+1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "    if nwords > 0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v_model.wv['india'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cheunghoyeung/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "word2vec_pred = []\n",
    "num_features = 100 \n",
    "for i in range(404290):\n",
    "    q1_avg_vector = avg_sentence_vector(sentences[0+i], w2v_model, num_features, \n",
    "                                        w2v_model.wv.vocab).reshape(1,-1)\n",
    "    q2_avg_vector = avg_sentence_vector(sentences[404290+i], w2v_model, num_features, \n",
    "                                        w2v_model.wv.vocab).reshape(1,-1)\n",
    "    score = cosine_similarity(q1_avg_vector,q2_avg_vector)\n",
    "    word2vec_pred.append(score[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec AUC: 0.7248271912128748\n",
      "Word2Vec accuracy: 0.5244700586212867\n"
     ]
    }
   ],
   "source": [
    "print('Word2Vec log_loss:', log_loss(train['is_duplicate'], word2vec_pred))\n",
    "print('Word2Vec accuracy:', accuracy_score(train['is_duplicate'], list(map(lambda x: round(x), word2vec_pred))))\n",
    "# AUC: lemma > token > stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
